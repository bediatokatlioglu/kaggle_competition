{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b1b3ee",
   "metadata": {},
   "source": [
    "# Afet Tweetleriyle Doğal Dil İşleme\n",
    "Hangi Tweetlerin gerçek felaketlerle ilgili olduğunu, hangilerinin olmadığını tahmin edin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b10285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410cf0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw=pd.read_csv(\"afettrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d85629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b33fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467cb55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf130ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd49934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bir NLP Projesine başlarken dikkat edilmesi gerekenler\n",
    "#### 1-Yazıdaki tüm harfleri küçük harfe çevir \n",
    "#### 2-Noktalama işaretlerini kaldır\n",
    "#### 3-Rakamları kaldır\n",
    "#### 4-Satır sonlarını kaldır\n",
    "#### 5-Gereksiz kelimeleri çıkart (Stopwords)\n",
    "#### 6-Tokenize et\n",
    "#### 7-Ekleri kaldır, kökleri bul (Lemma&Stemma)\n",
    "#### 8-Vektörize et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede69672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-küçük harfe çevireceğiz\n",
    "tw[\"text\"]=tw[\"text\"].str.lower()\n",
    "#2-noktalama işaretlerini kaldıracağız\n",
    "tw['text']=tw['text'].str.replace('[^\\w\\s]','')\n",
    "tw['text']=tw['text'].str.replace('\\n','')\n",
    "#3-rakamları kaldır\n",
    "tw['text']=tw['text'].str.replace('\\d+','')\n",
    "#4-satır başlarını kaldır\n",
    "tw['text']=tw['text'].str.replace('\\r','')\n",
    "tw['text'].replace({'r\"[\\s]+\"':''},regex=True,inplace=True)\n",
    "#Gereksiz boşlukları(whitespaces) kaldırdık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cfdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "tw['text']=tw['text'].apply(lambda text: normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n",
    "#Aksanları ortadan kaldırdık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9349d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria_ahrary thetawniest the out of control wil...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m  utckm s of volcano hawaii httptcozdtoydebj</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by northern califo...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7333 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target language  \n",
       "0     our deeds are the reason of this earthquake ma...       1       en  \n",
       "1                 forest fire near la ronge sask canada       1       en  \n",
       "2     all residents asked to shelter in place are be...       1       en  \n",
       "3      people receive wildfires evacuation orders in...       1       en  \n",
       "4     just got sent this photo from ruby alaska as s...       1       en  \n",
       "...                                                 ...     ...      ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1       en  \n",
       "7609  aria_ahrary thetawniest the out of control wil...       1       en  \n",
       "7610      m  utckm s of volcano hawaii httptcozdtoydebj       1       en  \n",
       "7611  police investigating after an ebike collided w...       1       en  \n",
       "7612  the latest more homes razed by northern califo...       1       en  \n",
       "\n",
       "[7333 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lang Detect\n",
    "from langdetect import detect\n",
    "tw['language']=tw['text'].apply(detect)\n",
    "tw=tw[tw['language']=='en']\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146b6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Gereksiz kelimeleri çıkart (Stopwords)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "tw['text']=tw['text'].apply(lambda x:\" \".join([i for i in str(x).split(\" \")  if i not in stop_words]))\n",
    "#Gereksiz kelimeleri kaldırdık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a01ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-Tokenize et\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tw['tokenized']=[tokenizer.tokenize(i) for i in tw['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f170035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-Ekleri kaldır, kökleri bul (Lemma&Stemma)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "tw['lemmatize']=[[lemmatizer.lemmatize(str(i)) for i in words] for words in tw['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee1572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pr=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9656dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmafn(text):\n",
    "    words=TextBlob(text).words\n",
    "    return [pr.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b856c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity / Subjectivity\n",
    "from textblob import TextBlob\n",
    "tw[['polarity','subjectivity']]=tw['text'].apply(lambda t:pd.Series(TextBlob(t).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b68b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>4464</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>not so cool KY</td>\n",
       "      <td>michael talking electrocuted omg rowysolouisvi...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[michael, talking, electrocuted, omg, rowysolo...</td>\n",
       "      <td>[m, i, c, h, a, e, l,  , t, a, l, k, i, n, g, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>752</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>colorado avalanche mens official colorado aval...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>[colorado, avalanche, mens, official, colorado...</td>\n",
       "      <td>[c, o, l, o, r, a, d, o,  , a, v, a, l, a, n, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>6402</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>Anderson, SC</td>\n",
       "      <td>hurricane sick</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[hurricane, sick]</td>\n",
       "      <td>[h, u, r, r, i, c, a, n, e,  , s, i, c, k]</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>7611</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>The P (South Philly)</td>\n",
       "      <td>pandemonium use fav cd  get httptcowhugaemc</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[pandemonium, use, fav, cd, get, httptcowhugaemc]</td>\n",
       "      <td>[p, a, n, d, e, m, o, n, i, u, m,  , u, s, e, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>6527</td>\n",
       "      <td>injuries</td>\n",
       "      <td>Georgia, U.S.A.</td>\n",
       "      <td>msnbc fucking idiot gun amp hatchet yet still ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[msnbc, fucking, idiot, gun, amp, hatchet, yet...</td>\n",
       "      <td>[m, s, n, b, c,  , f, u, c, k, i, n, g,  , i, ...</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>0.822222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>9022</td>\n",
       "      <td>stretcher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>invalid grazed towel stretcher pllolz witter c...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>[invalid, grazed, towel, stretcher, pllolz, wi...</td>\n",
       "      <td>[i, n, v, a, l, i, d,  , g, r, a, z, e, d,  , ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>2860</td>\n",
       "      <td>damage</td>\n",
       "      <td>My mind is my world</td>\n",
       "      <td>complaining phoenix mode fire emblem turns ray...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>[complaining, phoenix, mode, fire, emblem, tur...</td>\n",
       "      <td>[c, o, m, p, l, a, i, n, i, n, g,  , p, h, o, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>6946</td>\n",
       "      <td>massacre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cameron_wate parents colorado theater shooting...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[cameron_wate, parents, colorado, theater, sho...</td>\n",
       "      <td>[c, a, m, e, r, o, n, _, w, a, t, e,  , p, a, ...</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>6748</td>\n",
       "      <td>lava</td>\n",
       "      <td>Vancouver, BC</td>\n",
       "      <td>tried making chocolate peanut butter lava cake...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>[tried, making, chocolate, peanut, butter, lav...</td>\n",
       "      <td>[t, r, i, e, d,  , m, a, k, i, n, g,  , c, h, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword              location  \\\n",
       "3110  4464  electrocuted        not so cool KY   \n",
       "520    752     avalanche                   NaN   \n",
       "4504  6402     hurricane          Anderson, SC   \n",
       "5332  7611   pandemonium  The P (South Philly)   \n",
       "4589  6527      injuries       Georgia, U.S.A.   \n",
       "6311  9022     stretcher                   NaN   \n",
       "1988  2860        damage   My mind is my world   \n",
       "4878  6946      massacre                   NaN   \n",
       "4744  6748          lava         Vancouver, BC   \n",
       "\n",
       "                                                   text  target language  \\\n",
       "3110  michael talking electrocuted omg rowysolouisvi...       1       en   \n",
       "520   colorado avalanche mens official colorado aval...       0       en   \n",
       "4504                                     hurricane sick       1       en   \n",
       "5332        pandemonium use fav cd  get httptcowhugaemc       1       en   \n",
       "4589  msnbc fucking idiot gun amp hatchet yet still ...       1       en   \n",
       "6311  invalid grazed towel stretcher pllolz witter c...       0       en   \n",
       "1988  complaining phoenix mode fire emblem turns ray...       0       en   \n",
       "4878  cameron_wate parents colorado theater shooting...       1       en   \n",
       "4744  tried making chocolate peanut butter lava cake...       0       en   \n",
       "\n",
       "                                              tokenized  \\\n",
       "3110  [michael, talking, electrocuted, omg, rowysolo...   \n",
       "520   [colorado, avalanche, mens, official, colorado...   \n",
       "4504                                  [hurricane, sick]   \n",
       "5332  [pandemonium, use, fav, cd, get, httptcowhugaemc]   \n",
       "4589  [msnbc, fucking, idiot, gun, amp, hatchet, yet...   \n",
       "6311  [invalid, grazed, towel, stretcher, pllolz, wi...   \n",
       "1988  [complaining, phoenix, mode, fire, emblem, tur...   \n",
       "4878  [cameron_wate, parents, colorado, theater, sho...   \n",
       "4744  [tried, making, chocolate, peanut, butter, lav...   \n",
       "\n",
       "                                              lemmatize  polarity  \\\n",
       "3110  [m, i, c, h, a, e, l,  , t, a, l, k, i, n, g, ...  0.000000   \n",
       "520   [c, o, l, o, r, a, d, o,  , a, v, a, l, a, n, ...  0.000000   \n",
       "4504         [h, u, r, r, i, c, a, n, e,  , s, i, c, k] -0.714286   \n",
       "5332  [p, a, n, d, e, m, o, n, i, u, m,  , u, s, e, ...  0.000000   \n",
       "4589  [m, s, n, b, c,  , f, u, c, k, i, n, g,  , i, ... -0.211111   \n",
       "6311  [i, n, v, a, l, i, d,  , g, r, a, z, e, d,  , ...  0.000000   \n",
       "1988  [c, o, m, p, l, a, i, n, i, n, g,  , p, h, o, ...  0.000000   \n",
       "4878  [c, a, m, e, r, o, n, _, w, a, t, e,  , p, a, ... -0.075000   \n",
       "4744  [t, r, i, e, d,  , m, a, k, i, n, g,  , c, h, ...  0.000000   \n",
       "\n",
       "      subjectivity  \n",
       "3110      0.000000  \n",
       "520       0.100000  \n",
       "4504      0.857143  \n",
       "5332      0.000000  \n",
       "4589      0.822222  \n",
       "6311      0.000000  \n",
       "1988      0.000000  \n",
       "4878      0.050000  \n",
       "4744      0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.sample(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dccc266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[d, e, e, d, s,  , r, e, a, s, o, n,  , e, a, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[f, o, r, e, s, t,  , f, i, r, e,  , n, e, a, ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[r, e, s, i, d, e, n, t, s,  , a, s, k, e, d, ...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[ , p, e, o, p, l, e,  , r, e, c, e, i, v, e, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[g, o, t,  , s, e, n, t,  , p, h, o, t, o,  , ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[two, giant, cranes, holding, bridge, collapse...</td>\n",
       "      <td>[t, w, o,  , g, i, a, n, t,  , c, r, a, n, e, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria_ahrary thetawniest control wild fires cal...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[aria_ahrary, thetawniest, control, wild, fire...</td>\n",
       "      <td>[a, r, i, a, _, a, h, r, a, r, y,  , t, h, e, ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>utckm volcano hawaii httptcozdtoydebj</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[utckm, volcano, hawaii, httptcozdtoydebj]</td>\n",
       "      <td>[ , u, t, c, k, m,  , v, o, l, c, a, n, o,  , ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[police, investigating, ebike, collided, car, ...</td>\n",
       "      <td>[p, o, l, i, c, e,  , i, n, v, e, s, t, i, g, ...</td>\n",
       "      <td>-0.260417</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>[latest, homes, razed, northern, california, w...</td>\n",
       "      <td>[l, a, t, e, s, t,  , h, o, m, e, s,  , r, a, ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7333 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target language  \\\n",
       "0          deeds reason earthquake may allah forgive us       1       en   \n",
       "1                 forest fire near la ronge sask canada       1       en   \n",
       "2     residents asked shelter place notified officer...       1       en   \n",
       "3      people receive wildfires evacuation orders ca...       1       en   \n",
       "4     got sent photo ruby alaska smoke wildfires pou...       1       en   \n",
       "...                                                 ...     ...      ...   \n",
       "7608  two giant cranes holding bridge collapse nearb...       1       en   \n",
       "7609  aria_ahrary thetawniest control wild fires cal...       1       en   \n",
       "7610              utckm volcano hawaii httptcozdtoydebj       1       en   \n",
       "7611  police investigating ebike collided car little...       1       en   \n",
       "7612  latest homes razed northern california wildfir...       1       en   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1         [forest, fire, near, la, ronge, sask, canada]   \n",
       "2     [residents, asked, shelter, place, notified, o...   \n",
       "3     [people, receive, wildfires, evacuation, order...   \n",
       "4     [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "...                                                 ...   \n",
       "7608  [two, giant, cranes, holding, bridge, collapse...   \n",
       "7609  [aria_ahrary, thetawniest, control, wild, fire...   \n",
       "7610         [utckm, volcano, hawaii, httptcozdtoydebj]   \n",
       "7611  [police, investigating, ebike, collided, car, ...   \n",
       "7612  [latest, homes, razed, northern, california, w...   \n",
       "\n",
       "                                              lemmatize  polarity  \\\n",
       "0     [d, e, e, d, s,  , r, e, a, s, o, n,  , e, a, ...  0.000000   \n",
       "1     [f, o, r, e, s, t,  , f, i, r, e,  , n, e, a, ...  0.100000   \n",
       "2     [r, e, s, i, d, e, n, t, s,  , a, s, k, e, d, ... -0.100000   \n",
       "3     [ , p, e, o, p, l, e,  , r, e, c, e, i, v, e, ...  0.000000   \n",
       "4     [g, o, t,  , s, e, n, t,  , p, h, o, t, o,  , ...  0.000000   \n",
       "...                                                 ...       ...   \n",
       "7608  [t, w, o,  , g, i, a, n, t,  , c, r, a, n, e, ...  0.000000   \n",
       "7609  [a, r, i, a, _, a, h, r, a, r, y,  , t, h, e, ...  0.100000   \n",
       "7610  [ , u, t, c, k, m,  , v, o, l, c, a, n, o,  , ...  0.000000   \n",
       "7611  [p, o, l, i, c, e,  , i, n, v, e, s, t, i, g, ... -0.260417   \n",
       "7612  [l, a, t, e, s, t,  , h, o, m, e, s,  , r, a, ...  0.500000   \n",
       "\n",
       "      subjectivity  sentiment  \n",
       "0         0.000000       -1.0  \n",
       "1         0.400000       -1.0  \n",
       "2         0.400000       -1.0  \n",
       "3         0.000000       -1.0  \n",
       "4         0.000000       -1.0  \n",
       "...            ...        ...  \n",
       "7608      1.000000        1.0  \n",
       "7609      0.400000       -1.0  \n",
       "7610      0.000000       -1.0  \n",
       "7611      0.583333        1.0  \n",
       "7612      0.900000        1.0  \n",
       "\n",
       "[7333 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yukarıda çıkan sonuçları tek rakama dönüştürelim\n",
    "tw.loc[tw['subjectivity']>0.5,'sentiment']=1 #pozitive\n",
    "tw.loc[tw['subjectivity']==0.5,'sentiment']=0 #neutral\n",
    "tw.loc[tw['subjectivity']<0.5,'sentiment']=-1 #negative\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b43e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    5150\n",
       " 1.0    1879\n",
       " 0.0     304\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#icmali hali\n",
    "tw.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "577828a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 8-Vektörize et\n",
    "vect=CountVectorizer(stop_words=\"english\",ngram_range=(1,2),max_features=10000,analyzer=lemmafn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85991603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelleme\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58851265",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tw.text\n",
    "y=tw.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a272c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb5276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1081d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=LogisticRegression()\n",
    "d=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fa2e005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "554041aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f601408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=l.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c982ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8072727272727273"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91193214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=d.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "619fe907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481818181818182"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "685d8487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: 0.6012269938650306\n",
      "LogisticRegression: 0.787321063394683\n",
      "RandomForestClassifier: 0.776414451261077\n",
      "SVC: 0.7995910020449898\n",
      "KNeighborsClassifier: 0.6230402181322426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fnc_all_classification_models(x, y):\n",
    "    # Veriyi eğitim ve test setlerine bölelim\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Sınıflandırıcıları tanımlayalım\n",
    "    classifiers = [\n",
    "        GaussianNB(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "        SVC(),\n",
    "        KNeighborsClassifier()\n",
    "    ]\n",
    "\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        classifier.fit(x_train.toarray(), y_train)  # Seyrek matrisi yoğun matrise çevir\n",
    "        predictions = classifier.predict(x_test.toarray())  # Seyrek matrisi yoğun matrise çevir\n",
    "        accuracy_scores.append((classifier.__class__.__name__, accuracy_score(y_test, predictions)))\n",
    "\n",
    "    return accuracy_scores\n",
    "\n",
    "# x ve y giriş özellikleriniz ve etiketleriniz ise\n",
    "accuracy_scores = fnc_all_classification_models(x, y)\n",
    "for classifier, accuracy in accuracy_scores:\n",
    "    print(f\"{classifier}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b710f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test verisini tahmin edelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12a7f635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_test = pd.read_csv(\"afettest.csv\")\n",
    "tw_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f51fea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b431c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03d36b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing (adapted to clean Twitter text)\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #gets the stopword list from the dedicated library and saves them\n",
    "tk = TweetTokenizer() #defines the object, whose method is called in the function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "        # Check if the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        # Return non-string input as-is or convert to string\n",
    "        return str(text) if text is not None else ''\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    # Remove emojis\n",
    "    text = emoji.emojize(text, variant='emoji_type')\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    words = tk.tokenize(text)\n",
    "    # Lemmatize the text\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    # Remove stop words\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    # Join the tokens back together\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6583422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Metin ön işleme adımlarını uygula (örneğin, küçük harfe çevirme, özel karakterleri temizleme, vb.)\n",
    "    # Burada gerçekleştirilecek işlemler projenin gereksinimlerine bağlı olacaktır.\n",
    "    processed_text = text.lower()  # Örneğin, metni küçük harfe çevirme\n",
    "    # Diğer metin ön işleme adımlarını buraya ekleyebilirsiniz.\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0e9cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_test['text'] = tw_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5fdc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tw_test' verilerini vektörleştirin\n",
    "x_valid = vect.transform(tw_test['text'])\n",
    "\n",
    "# Tahminleri yapın\n",
    "y_preds = l.predict(x_valid)\n",
    "\n",
    "# Tahminleri 'target' sütununa ekleyin\n",
    "tw_test['predicted_target'] = y_preds\n",
    "\n",
    "# Tahmin edilen 'target' sütununu içeren bir DataFrame oluşturun\n",
    "submission_df = tw_test[['id', 'predicted_target']]\n",
    "\n",
    "# Submission dosyasını CSV olarak kaydedin\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2584d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
